name: CI

on:
  push:
    branches:
      - main
  pull_request:

permissions:
  contents: read

jobs:
  test:
    environment: Secrets
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          pip install poetry

      - name: Install dependencies
        run: |
          poetry config virtualenvs.in-project true
          poetry install --no-interaction --no-ansi

      - name: Run tests
        run: poetry run python -m pytest tests/
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  lint:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          pip install poetry

      - name: Install dependencies
        run: |
          poetry config virtualenvs.in-project true
          poetry install --no-interaction --no-ansi

      - name: Lint (flake8 + black)
        run: |
          poetry run flake8 .
          poetry run black --check .

  e2e-performance:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: e2e-performance
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate test data
        run: python utils/generate_test_files.py

      - name: Start full stack for E2E tests
        run: docker compose -f docker-compose.e2e.yml up -d --build

      - name: Wait for services to be ready
        run: |
          echo "Waiting for services to start..."
          sleep 30
          # Health check
          curl --retry 10 --retry-delay 5 --retry-connrefused http://localhost/api/health || true

      - name: Run E2E performance tests
        run: python -m pytest tests/ -v --tb=short

      - name: Stop stack
        if: always()
        run: docker compose -f docker-compose.e2e.yml down -v

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-performance-results
          path: e2e-performance/results/
          retention-days: 30

  # frontend-e2e-tests:
  #   E2E tests disabled in CI - they require backend and consume expensive AI tokens
  #   Run locally with: npm run test:e2e
  #   See: frontend/e2e-tests/README.md

  frontend-performance:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Run performance tests
        run: npx playwright test tests/leaflet-map.spec.ts --grep "Performance" --reporter=json > performance-results.json || true
        env:
          NEXT_TELEMETRY_DISABLED: '1'
          CI: 'true'

      - name: Extract performance metrics
        if: always()
        run: |
          echo "## 🚀 Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Parse and display performance metrics from test output
          if [ -f performance-results.json ]; then
            echo "Performance tests completed. Check test output for detailed metrics." >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Performance results file not found" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add to summary
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Categories:" >> $GITHUB_STEP_SUMMARY
          echo "- ⏱️ Multi-layer rendering speed" >> $GITHUB_STEP_SUMMARY
          echo "- 🗑️ Layer removal performance" >> $GITHUB_STEP_SUMMARY
          echo "- 💾 Memory usage tracking" >> $GITHUB_STEP_SUMMARY
          echo "- 🎯 Bounds fitting optimization" >> $GITHUB_STEP_SUMMARY

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: frontend/performance-results.json
          retention-days: 30

      
      