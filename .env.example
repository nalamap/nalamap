# LLM Provider Selection (REQUIRED)
# Choose ONE provider: openai, azure, google, mistral, or deepseek
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4.1-mini

# Azure OpenAI Configuration
# Model is configured via AZURE_OPENAI_DEPLOYMENT
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-08-01-preview
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_DEPLOYMENT=your_deployment_name
AZURE_OPENAI_API_VERSION=2024-08-01-preview


# Google AI Configuration
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_MODEL=gemini-1.5-pro-latest

# Mistral AI Configuration
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest

# DeepSeek Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat

# Database Configuration
DATABASE_AZURE_URL=postgresql://ROU:Geow3ave@geoweave.postgres.database.azure.com:5432/geollm

# API Configuration
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api

# LangSmith Tracing (optional)
LANGSMITH_TRACING=false
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=your_project_name

# Embedding Progress Configuration
# Enable/disable smooth interpolation of embedding progress (default: false)
NEXT_PUBLIC_EMBEDDING_INTERPOLATION_ENABLED=false
# Polling interval in milliseconds for checking embedding status (default: 3000)
NEXT_PUBLIC_EMBEDDING_POLLING_INTERVAL_MS=3000
# Default velocity for interpolation in layers per second (default: 3)
NEXT_PUBLIC_EMBEDDING_DEFAULT_VELOCITY=3
