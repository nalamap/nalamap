# LLM Provider Selection (REQUIRED)
# Choose ONE provider: openai, azure, google, mistral, or deepseek
LLM_PROVIDER=openai

# Default LLM Provider (OPTIONAL)
# Override which provider appears first in the UI dropdown (selected by default)
# If not set, will use LLM_PROVIDER value. Only affects UI default selection order.
# Example: DEFAULT_LLM_PROVIDER=azure

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4.1-mini

# Azure OpenAI Configuration
# Model is configured via AZURE_OPENAI_DEPLOYMENT
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-08-01-preview
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_DEPLOYMENT=your_deployment_name
AZURE_OPENAI_API_VERSION=2024-08-01-preview


# Google AI Configuration
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_MODEL=gemini-1.5-pro-latest

# Mistral AI Configuration
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest

# DeepSeek Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat

# Moonshot AI (Kimi) Configuration
MOONSHOT_API_KEY=your_moonshot_api_key_here
# Available models: kimi-k2.5, kimi-k2-turbo, moonshot-v1-128k, etc.
# Default: kimi-k2.5

# xAI (Grok) Configuration
XAI_API_KEY=your_xai_api_key_here
# Available models: grok-2-latest, grok-2-vision-latest, grok-beta
# Default: grok-2-latest

# Database Configuration
DATABASE_AZURE_URL=postgresql://ROU:Geow3ave@geoweave.postgres.database.azure.com:5432/geollm

# Authentication Configuration
# Set AUTH_ENABLED=false to disable authentication entirely (no login/signup required).
# The app will operate with an anonymous user. Useful for local development or demos.
AUTH_ENABLED=true

# API Configuration
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api

# LangSmith Tracing (optional)
LANGSMITH_TRACING=false
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=your_project_name

# Embedding Progress Configuration
# Enable/disable smooth interpolation of embedding progress (default: false)
NEXT_PUBLIC_EMBEDDING_INTERPOLATION_ENABLED=false
# Polling interval in milliseconds for checking embedding status (default: 3000)
NEXT_PUBLIC_EMBEDDING_POLLING_INTERVAL_MS=3000
# Default velocity for interpolation in layers per second (default: 3)
NEXT_PUBLIC_EMBEDDING_DEFAULT_VELOCITY=3

# Agent Performance Configuration
# Maximum number of recent messages to keep in conversation context (default: 20)
# Set to 0 to disable message pruning. Higher values increase token usage and cost.
MESSAGE_WINDOW_SIZE=20

# Message management strategy for long conversations (default: summarize)
# - 'summarize': Use LLM-based conversation summarization. Older messages are
#   intelligently condensed into a summary, preserving context while reducing tokens.
# - 'prune': Simple window-based truncation. Only keeps the most recent messages;
#   older messages are discarded without summarization.
MESSAGE_MANAGEMENT_MODE=summarize
