"""Modern attribute tool tailored for automated agents.

This module reimagines the attribute exploration workflow with a fresh code
base that emphasises structured payloads, robust validation and actionable
follow-up hints.  The implementation deliberately avoids importing helper
functions from :mod:`services.tools.attribute_tools` so it can evolve
independently while covering the same family of operations (listing fields,
summary statistics, unique values, filtering, field selection, sorting,
dataset descriptions and value sampling).
"""

from __future__ import annotations

import json
import logging
import re
import uuid
from dataclasses import dataclass, field
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse

import geopandas as gpd
import pandas as pd
import requests
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.tools.base import InjectedToolCallId
from langgraph.prebuilt import InjectedState
from langgraph.types import Command
from shapely.geometry import mapping
from typing_extensions import Annotated

from core.config import BASE_URL, LOCAL_UPLOAD_DIR
from models.geodata import DataOrigin, DataType, GeoDataObject
from models.states import GeoDataAgentState
from services.ai.llm_config import get_llm
from services.storage.file_management import store_file
from services.tools.utils import match_layer_names

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def _jsonify(value: Any) -> Any:
    """Convert pandas/numpy scalars into JSON serialisable primitives."""

    if pd.isna(value):
        return None
    if hasattr(value, "item"):
        try:
            return value.item()
        except Exception:  # pragma: no cover - extremely defensive
            pass
    if isinstance(value, (pd.Timestamp, pd.Timedelta)):
        return value.isoformat()
    return value


def _geometry_column(gdf: gpd.GeoDataFrame) -> Optional[str]:
    geom = getattr(gdf, "geometry", None)
    return geom.name if geom is not None else None


def _slugify(text: str) -> str:
    base = text.lower().strip() if text else "attribute-result"
    base = re.sub(r"[^a-z0-9\-_ ]+", "", base)
    base = base.replace(" ", "-")
    base = re.sub(r"-+", "-", base).strip("-")
    return base or "attribute-result"


def _clean_layer_name(text: str) -> str:
    """Clean layer name by removing file extensions and special characters.

    Args:
        text: The layer name or title to clean

    Returns:
        Cleaned layer name without file extensions and special characters

    Examples:
        >>> _clean_layer_name("Protected Area.geojson")
        'Protected Area'
        >>> _clean_layer_name("My-Layer_123.GeoJSON")
        'My-Layer_123'
        >>> _clean_layer_name("Layer (filtered).geojson")
        'Layer (filtered)'
    """
    if not text:
        return ""

    # Remove common GIS file extensions (case-insensitive)
    cleaned = re.sub(r"\.(geojson|json|shp|kml|gpkg|gml)$", "", text, flags=re.IGNORECASE)

    # Strip whitespace
    cleaned = cleaned.strip()

    return cleaned


def _generate_smart_layer_name(
    source_layer_name: str,
    operation: str,
    gdf: gpd.GeoDataFrame,
    operation_details: Optional[str] = None,
    state: Optional[GeoDataAgentState] = None,
) -> str:
    """Generate a smart, context-aware layer name using LLM.

    Uses dataset metadata, operation context, and user conversation to create
    meaningful layer names like "Al Houbara Protected Area Filtered" instead
    of generic names like "Protected Area (filtered)".

    Args:
        source_layer_name: Original layer name (cleaned)
        operation: Type of operation performed (e.g., "filtered", "sorted", "selected")
        gdf: GeoDataFrame with the data
        operation_details: Optional details about the operation (e.g., filter condition)
        state: Agent state with conversation context

    Returns:
        Smart layer name generated by LLM, or falls back to simple naming
    """
    # Fallback to simple naming with parentheses (matches current attribute_tool3 style)
    simple_name = f"{source_layer_name} ({operation})"

    try:
        # Get LLM instance
        llm = get_llm()

        # Build context from dataset
        row_count = len(gdf)
        columns = list(gdf.columns)

        # Try to extract relevant data from first few rows
        sample_data = {}
        if row_count > 0:
            # Get first row data (excluding geometry)
            first_row = gdf.iloc[0]
            geom_col = _geometry_column(gdf)
            for col in columns[:5]:  # Limit to first 5 columns
                if col != geom_col:
                    val = first_row.get(col)
                    if val is not None and str(val).strip():
                        sample_data[col] = str(val)[:100]  # Limit value length

        # Get user conversation context
        user_context = ""
        if state and "messages" in state:
            # Get last few messages for context
            recent_messages = state["messages"][-3:]
            user_context = " ".join(
                [msg.content for msg in recent_messages if hasattr(msg, "content")]
            )

        # Build LLM prompt
        system_prompt = (
            "You are a GIS data naming assistant. Generate a concise, descriptive layer name "
            "(2-5 words) based on the dataset context and operation. "
            "The name should be meaningful and help users identify the layer. "
            "Only respond with the layer name, nothing else."
        )

        context_info = {
            "source_name": source_layer_name,
            "operation": operation,
            "row_count": row_count,
            "sample_data": sample_data,
        }

        if operation_details:
            context_info["operation_details"] = operation_details

        human_prompt = f"""Dataset context: {json.dumps(context_info, indent=2)}

User context: {user_context[:300] if user_context else 'Not available'}

Generate a concise layer name (2-5 words) for this {operation} layer."""

        # Invoke LLM
        msgs = [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]
        resp = llm.generate([msgs])
        smart_name = resp.generations[0][0].text.strip()

        # Clean up the response (remove quotes, extra whitespace, etc.)
        smart_name = smart_name.strip("\"'").strip()

        # Validate the response - should be reasonable length
        if smart_name and 2 <= len(smart_name.split()) <= 8 and len(smart_name) <= 100:
            return smart_name

    except Exception as e:
        logger.debug(f"Failed to generate smart layer name: {e}. Using simple naming.")

    return simple_name


# ---------------------------------------------------------------------------
# Dataset loading
# ---------------------------------------------------------------------------


class GeoDataLoader:
    """Resolve a GeoDataObject into a GeoDataFrame.

    The loader understands local uploads, direct file paths and remote HTTP
    resources (including WFS endpoints where it enforces an explicit
    ``EPSG:4326`` projection).  The implementation is intentionally fresh and
    does not rely on the legacy attribute tool helpers.
    """

    def __init__(
        self,
        *,
        base_url: str = BASE_URL,
        upload_dir: str = LOCAL_UPLOAD_DIR,
        http_session: Optional[requests.Session] = None,
    ) -> None:
        self._base_url = base_url.rstrip("/") if base_url else ""
        self._upload_dir = Path(upload_dir or ".")
        self._session = http_session or requests.Session()

    # Public API ---------------------------------------------------------
    def load(self, layer: GeoDataObject) -> gpd.GeoDataFrame:
        link = layer.data_link
        if not link:
            raise IOError("Layer has no data link")

        # 1) base-url uploads (legacy local files)
        if self._base_url and link.startswith(self._base_url):
            local_candidate = self._resolve_base_url_path(link)
            if local_candidate is not None:
                return gpd.read_file(local_candidate)

        # 2) direct file path
        local_path = Path(link)
        if local_path.exists():
            return gpd.read_file(local_path)

        # 3) http(s)
        parsed = urlparse(link)
        if parsed.scheme in {"http", "https"}:
            request_url = self._ensure_wfs_projection(parsed)
            response = self._session.get(request_url, timeout=45)
            response.raise_for_status()
            return gpd.read_file(BytesIO(response.content))

        raise IOError(f"Unsupported data link: {link}")

    # Internal helpers ---------------------------------------------------
    def _resolve_base_url_path(self, link: str) -> Optional[Path]:
        path = urlparse(link).path
        filename = Path(path).name
        if not filename:
            return None
        candidate = self._upload_dir / filename
        if candidate.exists():
            return candidate
        return None

    def _ensure_wfs_projection(self, parsed) -> str:
        params = dict(parse_qsl(parsed.query, keep_blank_values=True))
        if "service" in params and params["service"].upper() == "WFS":
            params.setdefault("srsName", "EPSG:4326")
        elif "wfs" in parsed.path.lower():
            params.setdefault("srsName", "EPSG:4326")
        new_query = urlencode(params, doseq=True)
        return urlunparse(
            (
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                new_query,
                parsed.fragment,
            )
        )


# ---------------------------------------------------------------------------
# Schema / dataset description builders
# ---------------------------------------------------------------------------


@dataclass
class ColumnSummary:
    name: str
    dtype: str
    example: Any
    null_fraction: float
    stats: Dict[str, Any] = field(default_factory=dict)


class SchemaSummariser:
    """Produce lightweight schema metadata for planners and payloads."""

    def __init__(self, gdf: gpd.GeoDataFrame) -> None:
        self._gdf = gdf
        self._geom_col = _geometry_column(gdf)

    def describe(self, sample_rows: int = 5) -> Dict[str, Any]:
        head = self._gdf.head(sample_rows)
        summaries: List[ColumnSummary] = []
        for column in head.columns:
            series = head[column]
            dtype = "geometry" if column == self._geom_col else str(series.dtype)
            non_null = series.dropna()
            example = _jsonify(non_null.iloc[0]) if not non_null.empty else None
            null_fraction = float(series.isna().mean()) if len(series) else 0.0
            stats: Dict[str, Any] = {}
            if column != self._geom_col and pd.api.types.is_numeric_dtype(self._gdf[column]):
                numeric = pd.to_numeric(self._gdf[column], errors="coerce").dropna()
                if not numeric.empty:
                    stats = {
                        "min": float(numeric.min()),
                        "max": float(numeric.max()),
                        "mean": float(numeric.mean()),
                    }
            summaries.append(
                ColumnSummary(
                    name=column,
                    dtype=dtype,
                    example=example,
                    null_fraction=round(null_fraction, 4),
                    stats=stats,
                )
            )
        bbox = None
        try:
            minx, miny, maxx, maxy = self._gdf.total_bounds
            if all(pd.notna([minx, miny, maxx, maxy])):
                bbox = [float(minx), float(miny), float(maxx), float(maxy)]
        except Exception:  # pragma: no cover - shapely/geopandas edge case
            bbox = None

        geometry_counts = {}
        if self._geom_col and not self._gdf.empty:
            geometry_counts = (
                self._gdf.geometry.geom_type.fillna("Unknown").value_counts().to_dict()
            )

        return {
            "row_count": int(len(self._gdf)),
            "column_count": len(self._gdf.columns),
            "geometry_column": self._geom_col,
            "geometry_types": geometry_counts,
            "bounding_box": bbox,
            "columns": [summary.__dict__ for summary in summaries],
        }


# ---------------------------------------------------------------------------
# Planning layer
# ---------------------------------------------------------------------------


@dataclass
class PlannedOperation:
    operation: str
    params: Dict[str, Any]
    result_handling: str


class AttributePlanner:
    """Keyword driven planner for attribute operations."""

    def __init__(
        self,
        query: str,
        schema: Dict[str, Any],
    ) -> None:
        self._query = (query or "").lower()
        self._schema = schema

    def plan(self) -> PlannedOperation:
        if not self._query.strip():
            return PlannedOperation("describe_dataset", {}, "chat")

        if "list" in self._query and "field" in self._query:
            return PlannedOperation("list_fields", {}, "chat")

        if "describe" in self._query or "overview" in self._query:
            return PlannedOperation("describe_dataset", {}, "chat")

        if any(word in self._query for word in ["summary", "summarise", "statistics", "average"]):
            fields = self._extract_fields()
            return PlannedOperation("summarize", {"fields": fields}, "chat")

        if any(word in self._query for word in ["unique", "distinct"]):
            fields = self._extract_fields()
            field = fields[0] if fields else self._first_non_geometry_field()
            return PlannedOperation("unique_values", {"field": field}, "chat")

        if any(word in self._query for word in ["filter", "where", "only", "show"]):
            where = self._extract_condition()
            return PlannedOperation(
                "filter_where",
                {"where": where or ""},
                "layer",
            )

        if "select" in self._query and "field" in self._query:
            fields = self._extract_fields()
            return PlannedOperation("select_fields", {"include": fields}, "layer")

        if any(word in self._query for word in ["sort", "order", "ascending", "descending"]):
            field = self._extract_fields()
            first = field[0] if field else self._first_non_geometry_field()
            order = "desc" if "desc" in self._query else "asc"
            return PlannedOperation("sort_by", {"fields": [[first, order]]}, "layer")

        if "value" in self._query and "row" in self._query:
            columns = self._extract_fields()
            return PlannedOperation("get_attribute_values", {"columns": columns}, "chat")

        return PlannedOperation("describe_dataset", {}, "chat")

    # Helpers ------------------------------------------------------------
    def _extract_fields(self) -> List[str]:
        candidates = [c["name"] for c in self._schema.get("columns", [])]
        results: List[str] = []
        for name in candidates:
            if not name:
                continue
            pattern = r"\b" + re.escape(name.lower()) + r"\b"
            if re.search(pattern, self._query):
                results.append(name)
        return results[:4]

    def _first_non_geometry_field(self) -> Optional[str]:
        geom = self._schema.get("geometry_column")
        for column in self._schema.get("columns", []):
            if column["name"] != geom:
                return column["name"]
        return None

    def _extract_condition(self) -> Optional[str]:
        match = re.search(r"where (.+)", self._query)
        if match:
            return match.group(1)
        match = re.search(r"only (.+)", self._query)
        if match:
            return match.group(1)
        return None


# ---------------------------------------------------------------------------
# Execution layer
# ---------------------------------------------------------------------------


class OperationError(Exception):
    def __init__(self, message: str, *, details: Optional[Dict[str, Any]] = None) -> None:
        super().__init__(message)
        self.details = details or {}


@dataclass
class ExecutionOutcome:
    status: str
    result: Dict[str, Any]
    result_handling: str
    new_geodata: Optional[gpd.GeoDataFrame] = None
    keep_geometry: bool = True
    title_suffix: Optional[str] = None
    detailed_description: Optional[str] = None
    field_suggestions: Optional[List[str]] = None
    next_actions: List[Dict[str, Any]] = field(default_factory=list)


class AttributeExecutor:
    def __init__(self, gdf: gpd.GeoDataFrame) -> None:
        self._gdf = gdf
        self._geom_col = _geometry_column(gdf)

    # Public API ---------------------------------------------------------
    def execute(self, operation: str, params: Dict[str, Any]) -> ExecutionOutcome:
        handler_name = f"_op_{operation}"
        handler = getattr(self, handler_name, None)
        if handler is None:
            raise OperationError(
                f"Unsupported operation '{operation}'", details={"operation": operation}
            )
        return handler(params)

    # Individual operations ---------------------------------------------
    def _op_list_fields(self, params: Dict[str, Any]) -> ExecutionOutcome:
        fields: List[Dict[str, Any]] = []
        for column in self._gdf.columns:
            series = self._gdf[column]
            example = None
            non_null = series.dropna()
            if column != self._geom_col and not non_null.empty:
                example = _jsonify(non_null.iloc[0])
            fields.append(
                {
                    "name": column,
                    "dtype": "geometry" if column == self._geom_col else str(series.dtype),
                    "non_null": int(series.notna().sum()),
                    "nulls": int(series.isna().sum()),
                    "example": example,
                }
            )
        return ExecutionOutcome(
            status="success",
            result={"fields": fields},
            result_handling="chat",
        )

    def _validate_fields(self, requested: Sequence[str]) -> List[str]:
        available = set(self._gdf.columns)
        return [field for field in requested if field not in available]

    def _numeric_summary(self, field: str) -> Dict[str, Any]:
        series = pd.to_numeric(self._gdf[field], errors="coerce")
        series = series.dropna()
        if series.empty:
            raise OperationError(
                f"Field '{field}' is not numeric or has no numeric values",
                details={"field": field},
            )
        return {
            "count": int(series.count()),
            "min": float(series.min()),
            "max": float(series.max()),
            "mean": float(series.mean()),
            "median": float(series.median()),
            "std": float(series.std(ddof=0)),
        }

    def _op_summarize(self, params: Dict[str, Any]) -> ExecutionOutcome:
        fields = params.get("fields") or []
        if not fields:
            fields = [
                c
                for c in self._gdf.columns
                if c != self._geom_col and pd.api.types.is_numeric_dtype(self._gdf[c])
            ][:3]
        missing = self._validate_fields(fields)
        if missing:
            raise OperationError(
                "Unknown field(s) for summarize", details={"missing_fields": missing}
            )
        summary = {field: self._numeric_summary(field) for field in fields}
        return ExecutionOutcome(
            status="success",
            result=summary,
            result_handling="chat",
        )

    def _op_unique_values(self, params: Dict[str, Any]) -> ExecutionOutcome:
        field = params.get("field")
        if not field:
            raise OperationError("Parameter 'field' is required for unique_values")
        missing = self._validate_fields([field])
        if missing:
            raise OperationError(
                "Unknown field for unique_values", details={"missing_fields": missing}
            )
        top_k = params.get("top_k") or 20
        series = self._gdf[field]
        counts = series.value_counts(dropna=False).head(top_k)
        values = [{"value": _jsonify(idx), "count": int(count)} for idx, count in counts.items()]
        return ExecutionOutcome(
            status="success",
            result={
                "field": field,
                "values": values,
                "total_unique": int(series.nunique(dropna=False)),
            },
            result_handling="chat",
        )

    def _apply_query(self, where: str) -> Tuple[gpd.GeoDataFrame, List[str]]:
        if not where:
            raise OperationError("Filter condition is empty")
        try:
            filtered = self._gdf.query(where, engine="python")
        except Exception as exc:
            suggestions = self._suggest_fields(where)
            raise OperationError(
                f"Failed to apply filter: {exc}",
                details={"suggestions": suggestions},
            ) from exc
        suggestions = self._suggest_fields(where)
        return filtered, suggestions

    def _suggest_fields(self, expression: str) -> List[str]:
        tokens = re.findall(r"[A-Za-z_][A-Za-z0-9_]*", expression)
        suggestions: List[str] = []
        for token in tokens:
            if token in self._gdf.columns:
                suggestions.append(token)
        return sorted(set(suggestions))

    def _op_filter_where(self, params: Dict[str, Any]) -> ExecutionOutcome:
        where = params.get("where") or ""
        filtered, suggestions = self._apply_query(where)
        if len(filtered) == 0:
            return ExecutionOutcome(
                status="success",
                result={
                    "type": "filter",
                    "feature_count": 0,
                    "original_feature_count": int(len(self._gdf)),
                    "message": "Filter applied but no features matched the condition.",
                },
                result_handling="chat",
                field_suggestions=suggestions,
            )

        # Create detailed description
        detailed_desc = (
            f"Filtered features using condition: {where}. "
            f"Result contains {len(filtered)} feature(s) out of "
            f"{len(self._gdf)} original features."
        )

        return ExecutionOutcome(
            status="success",
            result={
                "type": "filter",
                "feature_count": int(len(filtered)),
                "original_feature_count": int(len(self._gdf)),
            },
            result_handling="layer",
            new_geodata=filtered,
            title_suffix="filtered",
            detailed_description=detailed_desc,
            field_suggestions=suggestions,
            next_actions=[
                {
                    "type": "register_result_layer",
                    "reason": "Filtered subset of layer",
                }
            ],
        )

    def _op_select_fields(self, params: Dict[str, Any]) -> ExecutionOutcome:
        include: Optional[Sequence[str]] = params.get("include")
        exclude: Optional[Sequence[str]] = params.get("exclude")
        keep_geometry = params.get("keep_geometry", True)
        to_validate = list(include or []) + list(exclude or [])
        missing = self._validate_fields(to_validate)
        if missing:
            raise OperationError(
                "Unknown field(s) for select_fields", details={"missing_fields": missing}
            )
        columns = list(include) if include else list(self._gdf.columns)
        if exclude:
            columns = [c for c in columns if c not in exclude]
        if keep_geometry and self._geom_col and self._geom_col not in columns:
            columns.append(self._geom_col)
        result = self._gdf[columns].copy()
        if not keep_geometry and self._geom_col in result.columns:
            result = result.drop(columns=[self._geom_col])

        # Create detailed description
        field_info = []
        if include:
            field_info.append(f"included fields: {', '.join(include)}")
        if exclude:
            field_info.append(f"excluded fields: {', '.join(exclude)}")
        detailed_desc = (
            f"Selected fields. "
            f"{'; '.join(field_info) if field_info else 'Field selection applied'}. "
            f"Result has {len(result.columns)} columns."
        )

        return ExecutionOutcome(
            status="success",
            result={"columns": columns, "original_column_count": len(self._gdf.columns)},
            result_handling="layer",
            new_geodata=result,
            keep_geometry=keep_geometry,
            title_suffix="selected-fields",
            detailed_description=detailed_desc,
            next_actions=[
                {
                    "type": "register_result_layer",
                    "reason": "Subset of columns",
                }
            ],
        )

    def _op_sort_by(self, params: Dict[str, Any]) -> ExecutionOutcome:
        fields = params.get("fields") or []
        if not fields:
            raise OperationError("Parameter 'fields' is required for sort_by")
        to_validate = [f[0] if isinstance(f, (list, tuple)) else f for f in fields]
        missing = self._validate_fields(to_validate)
        if missing:
            raise OperationError(
                "Unknown field(s) for sort_by", details={"missing_fields": missing}
            )
        sort_keys = []
        ascending = []
        for item in fields:
            if isinstance(item, (list, tuple)):
                field_name, order = item[0], item[1] if len(item) > 1 else "asc"
            elif isinstance(item, dict):
                field_name = item.get("name")
                order = item.get("order", "asc")
            else:
                field_name, order = item, "asc"
            sort_keys.append(field_name)
            ascending.append(order.lower() != "desc")
        result = self._gdf.sort_values(by=sort_keys, ascending=ascending)

        # Create detailed description
        sort_desc = ", ".join(
            [f"{key} {'asc' if asc else 'desc'}" for key, asc in zip(sort_keys, ascending)]
        )
        detailed_desc = (
            f"Sorted by: {sort_desc}. " f"Result contains {len(result)} features in sorted order."
        )

        return ExecutionOutcome(
            status="success",
            result={
                "sorted_by": sort_keys,
                "orders": ["asc" if asc else "desc" for asc in ascending],
            },
            result_handling="layer",
            new_geodata=result,
            title_suffix="sorted",
            detailed_description=detailed_desc,
            next_actions=[
                {
                    "type": "register_result_layer",
                    "reason": "Sorted view of layer",
                }
            ],
        )

    def _describe_actions(self) -> List[str]:
        actions: List[str] = []
        if any(
            pd.api.types.is_numeric_dtype(self._gdf[c])
            for c in self._gdf.columns
            if c != self._geom_col
        ):
            actions.append("Create thematic styling by numeric ranges")
        if self._geom_col:
            actions.append("Perform spatial join with another layer")
        if len(self._gdf) > 500:
            actions.append("Consider aggregating features by category")
        return actions

    def _op_describe_dataset(self, params: Dict[str, Any]) -> ExecutionOutcome:
        schema = SchemaSummariser(self._gdf).describe()
        sample_records = []
        for _, row in self._gdf.head(5).iterrows():
            record = {col: _jsonify(row[col]) for col in self._gdf.columns if col != self._geom_col}
            sample_records.append(record)
        schema["sample_records"] = sample_records
        schema["recommended_actions"] = self._describe_actions()
        return ExecutionOutcome(
            status="success",
            result=schema,
            result_handling="chat",
        )

    def _apply_row_filter(self, row_filter: Optional[str]) -> gpd.GeoDataFrame:
        if not row_filter:
            return self._gdf
        try:
            return self._gdf.query(row_filter, engine="python")
        except Exception as exc:
            raise OperationError(f"Failed to apply row filter: {exc}") from exc

    def _op_get_attribute_values(self, params: Dict[str, Any]) -> ExecutionOutcome:
        columns = params.get("columns") or [c for c in self._gdf.columns if c != self._geom_col][:4]
        missing = self._validate_fields(columns)
        if missing:
            raise OperationError(
                "Unknown field(s) for get_attribute_values", details={"missing_fields": missing}
            )
        filtered = self._apply_row_filter(params.get("row_filter"))
        sample = filtered[columns].head(params.get("limit", 10))
        rows = [
            {column: _jsonify(value) for column, value in record.items()}
            for record in sample.to_dict(orient="records")
        ]
        return ExecutionOutcome(
            status="success",
            result={"columns": columns, "rows": rows, "row_count": int(len(filtered))},
            result_handling="chat",
        )


# ---------------------------------------------------------------------------
# Result persistence
# ---------------------------------------------------------------------------


class ResultWriter:
    def __init__(self, source_layer: GeoDataObject) -> None:
        self._source = source_layer

    def persist(
        self,
        gdf: gpd.GeoDataFrame,
        *,
        title_suffix: str,
        keep_geometry: bool,
        detailed_description: Optional[str] = None,
        operation_details: Optional[str] = None,
        state: Optional[GeoDataAgentState] = None,
    ) -> GeoDataObject:
        # Clean the source layer name/title to remove file extensions and special chars
        source_name = _clean_layer_name(self._source.title or self._source.name)

        # Generate smart layer name using LLM
        title = _generate_smart_layer_name(
            source_layer_name=source_name,
            operation=title_suffix,
            gdf=gdf,
            operation_details=operation_details,
            state=state,
        )

        slug = _slugify(title)
        feature_collection = self._feature_collection(gdf, keep_geometry=keep_geometry)
        content = json.dumps(feature_collection).encode("utf-8")
        filename = f"{slug}_{uuid.uuid4().hex[:8]}.geojson"
        url, _ = store_file(filename, content)

        # Use detailed description if provided, otherwise create a simple one
        description = (
            detailed_description
            if detailed_description
            else f"Result of attribute_tool3 operation on {self._source.name}"
        )
        llm_desc = detailed_description if detailed_description else title

        return GeoDataObject(
            id=uuid.uuid4().hex,
            data_source_id="attribute_tool3",
            data_type=DataType.GEOJSON,
            data_origin=DataOrigin.TOOL,
            data_source="NaLaMapAttribute3",
            data_link=url,
            name=slug,
            title=title,
            description=description,
            llm_description=llm_desc,
            properties={"source_layer_id": self._source.id},
        )

    def _feature_collection(
        self,
        gdf: gpd.GeoDataFrame,
        *,
        keep_geometry: bool,
    ) -> Dict[str, Any]:
        geom_col = _geometry_column(gdf)
        features: List[Dict[str, Any]] = []
        for _, row in gdf.iterrows():
            properties = {}
            for column in gdf.columns:
                if column == geom_col:
                    continue
                properties[column] = _jsonify(row[column])
            geometry = None
            if keep_geometry and geom_col:
                geometry = mapping(row[geom_col]) if row[geom_col] is not None else None
            features.append({"type": "Feature", "properties": properties, "geometry": geometry})
        return {"type": "FeatureCollection", "features": features}


# ---------------------------------------------------------------------------
# Tool orchestration
# ---------------------------------------------------------------------------


def _last_user_message(messages: Iterable[Any]) -> str:
    for message in reversed(list(messages)):
        if isinstance(message, HumanMessage):
            return message.content or ""
        if getattr(message, "type", None) == "human":
            return getattr(message, "content", "") or ""
    return ""


def _serialise_layer(layer: GeoDataObject) -> Dict[str, Any]:
    return {
        "id": layer.id,
        "name": layer.name,
        "title": layer.title,
        "data_source": layer.data_source,
        "data_link": layer.data_link,
        "data_type": layer.data_type,
    }


def _base_payload(
    *,
    layer: GeoDataObject,
    operation: str,
    params: Dict[str, Any],
    result_handling: str,
    status: str,
) -> Dict[str, Any]:
    return {
        "status": status,
        "operation": operation,
        "layer": _serialise_layer(layer),
        "params": params,
        "result_handling": result_handling,
    }


def _command_from_payload(
    payload: Dict[str, Any],
    *,
    tool_call_id: str,
    status: str,
    geodata_results: Optional[List[GeoDataObject]] = None,
) -> Command:
    message = ToolMessage(
        name="attribute_tool3",
        content=json.dumps(payload, default=str),
        tool_call_id=tool_call_id,
        status=status,
    )
    update: Dict[str, Any] = {"messages": [message]}
    if geodata_results is not None:
        update["geodata_results"] = geodata_results
    return Command(update=update)


def _ensure_layer(
    layers: Sequence[GeoDataObject], target_layer_names: Optional[List[str]]
) -> GeoDataObject:
    if not layers:
        raise OperationError("No geodata layers available")
    selected = (
        match_layer_names(layers, target_layer_names) if target_layer_names else list(layers[:1])
    )
    if not selected:
        raise OperationError(
            "Target layer(s) not found",
            details={
                "available_layers": [layer.name for layer in layers],
                "requested": target_layer_names,
            },
        )
    layer = selected[0]
    if layer.data_type not in {DataType.GEOJSON, DataType.UPLOADED}:
        raise OperationError(
            "Selected layer is not GeoJSON compatible",
            details={"data_type": layer.data_type},
        )
    return layer


@tool
def attribute_tool3(
    state: Annotated[GeoDataAgentState, InjectedState],
    tool_call_id: Annotated[str, InjectedToolCallId],
    target_layer_names: Optional[List[str]] = None,
    operation: Optional[str] = None,
    params: Optional[Dict[str, Any]] = None,
    result_handling: Optional[str] = None,
) -> Command:
    """Execute attribute operations and emit structured payloads."""

    layers = state.get("geodata_layers") or []
    messages = state.get("messages") or []

    try:
        layer = _ensure_layer(layers, target_layer_names)
    except OperationError as exc:
        payload = {
            "status": "error",
            "message": str(exc),
            "details": exc.details,
        }
        return _command_from_payload(payload, tool_call_id=tool_call_id, status="error")

    loader = GeoDataLoader()
    try:
        gdf = loader.load(layer)
    except Exception as exc:  # pragma: no cover - depends on IO
        logger.exception("Failed to load GeoDataFrame for attribute_tool3")
        payload = {
            "status": "error",
            "message": str(exc),
        }
        return _command_from_payload(payload, tool_call_id=tool_call_id, status="error")

    schema = SchemaSummariser(gdf).describe()

    if operation is None:
        planner = AttributePlanner(_last_user_message(messages), schema)
        planned = planner.plan()
        operation = planned.operation
        inferred_params = planned.params
        params = {**inferred_params, **(params or {})}
        result_handling = result_handling or planned.result_handling
    else:
        params = params or {}
    handling_was_auto = False
    if result_handling is None:
        result_handling = (
            "layer" if operation in {"filter_where", "select_fields", "sort_by"} else "chat"
        )
        handling_was_auto = True

    executor = AttributeExecutor(gdf)

    try:
        outcome = executor.execute(operation, params)
    except OperationError as exc:
        payload = _base_payload(
            layer=layer,
            operation=operation,
            params=params,
            result_handling=result_handling or "chat",
            status="error",
        )
        payload["message"] = str(exc)
        if exc.details:
            payload["details"] = exc.details
        return _command_from_payload(payload, tool_call_id=tool_call_id, status="error")

    final_result_handling = outcome.result_handling if handling_was_auto else result_handling

    payload = _base_payload(
        layer=layer,
        operation=operation,
        params=params,
        result_handling=final_result_handling,
        status=outcome.status,
    )
    payload["result"] = outcome.result
    if outcome.field_suggestions:
        payload["field_suggestions"] = outcome.field_suggestions
    if outcome.next_actions:
        payload["next_actions"] = outcome.next_actions

    geodata_results = state.get("geodata_results") or []
    if outcome.new_geodata is not None:
        writer = ResultWriter(layer)

        # Extract operation details for smart naming
        operation_details = None
        if operation == "filter_where" and params.get("where"):
            operation_details = params["where"]
        elif operation == "sort_by" and params.get("fields"):
            fields = params["fields"][:2]  # Limit to first 2 fields
            operation_details = ", ".join([f"{fld} {order}" for fld, order in fields])
        elif operation == "select_fields":
            include = params.get("include", [])
            exclude = params.get("exclude", [])
            details = []
            if include:
                details.append(f"included: {', '.join(include[:3])}")
            if exclude:
                details.append(f"excluded: {', '.join(exclude[:3])}")
            operation_details = "; ".join(details) if details else None

        new_layer = writer.persist(
            outcome.new_geodata,
            title_suffix=outcome.title_suffix or operation,
            keep_geometry=outcome.keep_geometry,
            detailed_description=outcome.detailed_description,
            operation_details=operation_details,
            state=state,
        )
        geodata_results = geodata_results + [new_layer]
        payload["result"]["new_layer"] = _serialise_layer(new_layer)
        payload["result"]["feature_count"] = int(len(outcome.new_geodata))

    return _command_from_payload(
        payload,
        tool_call_id=tool_call_id,
        status=outcome.status,
        geodata_results=geodata_results,
    )
